{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Version Evaluation with LangSmith\n",
    "\n",
    "This notebook demonstrates how to evaluate and compare two AI models/providers in terms of correctness and latency using LangSmith.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Set up your virtual environment (see README.md)\n",
    "2. Copy `.env.example` to `.env` and fill in your API keys\n",
    "3. Install dependencies: `pip install -r requirements.txt`\n",
    "\n",
    "## Overview\n",
    "\n",
    "This evaluation will:\n",
    "- Load a dataset from LangSmith\n",
    "- Run two different models on the same inputs\n",
    "- Measure correctness and latency\n",
    "- Compare results and generate visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path for imports\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('')), 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import custom utilities\n",
    "from src.config import validate_config\n",
    "from src.utils import measure_latency, calculate_metrics\n",
    "from src.evaluators import combined_evaluator\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    validate_config()\n",
    "    print(\"\u2713 Configuration validated successfully\")\n",
    "except ValueError as e:\n",
    "    print(f\"\u2717 Configuration error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain and LangSmith\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "print(\"\u2713 LangSmith client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Models to Compare\n",
    "\n",
    "We'll compare two different OpenAI models, but you can easily adapt this to compare different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure two models to compare\n",
    "model_1 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    tags=[\"model-1\", \"gpt-3.5\"]\n",
    ")\n",
    "\n",
    "model_2 = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0.7,\n",
    "    tags=[\"model-2\", \"gpt-4\"]\n",
    ")\n",
    "\n",
    "print(\"Model 1: GPT-3.5 Turbo\")\n",
    "print(\"Model 2: GPT-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset from LangSmith Traces (Optional)\n",
    "\n",
    "Instead of using a manual test dataset, you can create a dataset from your existing LangSmith traces that match certain filters. This is useful for:\n",
    "- Converting production traces into test datasets\n",
    "- Filtering traces by tags, metadata, or feedback scores\n",
    "- Creating regression test suites from real usage\n",
    "\n",
    "### Step 1: List and Filter Traces\n",
    "\n",
    "First, let's explore traces matching specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define filter criteria\n",
    "# You can filter by: project, tags, metadata, feedback scores, date range, etc.\n",
    "filter_params = {\n",
    "    # Filter by project name\n",
    "    # \"project_name\": \"my-production-project\",\n",
    "    \n",
    "    # Filter by tags (traces must have ALL specified tags)\n",
    "    # \"filter\": 'has(tags, \"production\") and has(tags, \"customer-query\")',\n",
    "    \n",
    "    # Filter by date range (e.g., last 7 days)\n",
    "    # \"start_time\": datetime.now() - timedelta(days=7),\n",
    "    # \"end_time\": datetime.now(),\n",
    "    \n",
    "    # Filter by feedback scores (e.g., only traces with positive feedback)\n",
    "    # \"filter\": 'eq(feedback_score, 1)',\n",
    "    \n",
    "    # Limit number of traces\n",
    "    \"limit\": 10\n",
    "}\n",
    "\n",
    "# List traces matching the filter\n",
    "print(\"Fetching traces from LangSmith...\")\n",
    "print(f\"Filters: {filter_params}\")\n",
    "print(\"\\nNote: Uncomment and customize the filter parameters above to match your needs\")\n",
    "\n",
    "# Fetch traces\n",
    "try:\n",
    "    runs = list(client.list_runs(**filter_params))\n",
    "    print(f\"\\n\u2713 Found {len(runs)} traces matching the filter\")\n",
    "    \n",
    "    # Display first few traces\n",
    "    if runs:\n",
    "        print(\"\\nExample traces:\")\n",
    "        for i, run in enumerate(runs[:3]):\n",
    "            print(f\"\\nTrace {i+1}:\")\n",
    "            print(f\"  ID: {run.id}\")\n",
    "            print(f\"  Name: {run.name}\")\n",
    "            print(f\"  Tags: {run.tags if hasattr(run, 'tags') else 'N/A'}\")\n",
    "            print(f\"  Start time: {run.start_time}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error fetching traces: {e}\")\n",
    "    print(\"\\nMake sure you have traces in your LangSmith project.\")\n",
    "    runs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert Traces to Dataset Format\n",
    "\n",
    "Extract inputs and outputs from the filtered traces to create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset_from_traces(runs):\n",
    "    \"\"\"\n",
    "    Extract inputs and outputs from LangSmith traces.\n",
    "    \n",
    "    Args:\n",
    "        runs: List of LangSmith run objects\n",
    "    \n",
    "    Returns:\n",
    "        list: Dataset in the format [{\"input\": ..., \"expected_output\": ...}, ...]\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    for run in runs:\n",
    "        try:\n",
    "            # Extract input - adjust based on your trace structure\n",
    "            # Common patterns:\n",
    "            # - run.inputs.get(\"input\") or run.inputs.get(\"question\")\n",
    "            # - run.inputs.get(\"messages\")[0][\"content\"] for chat models\n",
    "            input_data = None\n",
    "            if hasattr(run, 'inputs') and run.inputs:\n",
    "                # Try common input keys\n",
    "                input_data = (\n",
    "                    run.inputs.get(\"input\") or \n",
    "                    run.inputs.get(\"question\") or\n",
    "                    run.inputs.get(\"query\") or\n",
    "                    run.inputs.get(\"text\")\n",
    "                )\n",
    "                \n",
    "                # For chat models with messages\n",
    "                if not input_data and \"messages\" in run.inputs:\n",
    "                    messages = run.inputs[\"messages\"]\n",
    "                    if messages and len(messages) > 0:\n",
    "                        input_data = messages[-1].get(\"content\", messages[-1])\n",
    "            \n",
    "            # Extract output - adjust based on your trace structure\n",
    "            output_data = None\n",
    "            if hasattr(run, 'outputs') and run.outputs:\n",
    "                # Try common output keys\n",
    "                output_data = (\n",
    "                    run.outputs.get(\"output\") or\n",
    "                    run.outputs.get(\"answer\") or\n",
    "                    run.outputs.get(\"result\") or\n",
    "                    run.outputs.get(\"text\")\n",
    "                )\n",
    "                \n",
    "                # For chat models with content\n",
    "                if not output_data and \"content\" in run.outputs:\n",
    "                    output_data = run.outputs[\"content\"]\n",
    "            \n",
    "            # Only add if we have both input and output\n",
    "            if input_data and output_data:\n",
    "                dataset.append({\n",
    "                    \"input\": str(input_data),\n",
    "                    \"expected_output\": str(output_data),\n",
    "                    \"trace_id\": str(run.id)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract data from trace {run.id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Convert traces to dataset\n",
    "if runs:\n",
    "    trace_dataset = extract_dataset_from_traces(runs)\n",
    "    print(f\"\u2713 Extracted {len(trace_dataset)} examples from traces\")\n",
    "    \n",
    "    # Display first example\n",
    "    if trace_dataset:\n",
    "        print(\"\\nExample from trace dataset:\")\n",
    "        print(f\"Input: {trace_dataset[0]['input'][:100]}...\" if len(trace_dataset[0]['input']) > 100 else f\"Input: {trace_dataset[0]['input']}\")\n",
    "        print(f\"Expected: {trace_dataset[0]['expected_output'][:100]}...\" if len(trace_dataset[0]['expected_output']) > 100 else f\"Expected: {trace_dataset[0]['expected_output']}\")\n",
    "else:\n",
    "    print(\"No traces available. Using manual test dataset instead.\")\n",
    "    trace_dataset = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create LangSmith Dataset (Optional)\n",
    "\n",
    "You can save the filtered traces as a reusable dataset in LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset in LangSmith\n",
    "dataset_name = \"my-evaluation-dataset\"\n",
    "\n",
    "if trace_dataset:\n",
    "    try:\n",
    "        # Check if dataset already exists\n",
    "        try:\n",
    "            existing_dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "            print(f\"Dataset '{dataset_name}' already exists with {existing_dataset.example_count} examples\")\n",
    "            print(\"Skipping dataset creation. To create a new one, change the dataset_name.\")\n",
    "        except:\n",
    "            # Dataset doesn't exist, create it\n",
    "            dataset = client.create_dataset(\n",
    "                dataset_name=dataset_name,\n",
    "                description=\"Dataset created from filtered LangSmith traces for model evaluation\"\n",
    "            )\n",
    "            print(f\"\u2713 Created dataset '{dataset_name}'\")\n",
    "            \n",
    "            # Add examples to the dataset\n",
    "            for example in trace_dataset:\n",
    "                client.create_example(\n",
    "                    inputs={\"input\": example[\"input\"]},\n",
    "                    outputs={\"output\": example[\"expected_output\"]},\n",
    "                    dataset_id=dataset.id,\n",
    "                    metadata={\"source_trace_id\": example.get(\"trace_id\")}\n",
    "                )\n",
    "            \n",
    "            print(f\"\u2713 Added {len(trace_dataset)} examples to dataset\")\n",
    "            print(f\"\\nView your dataset at: https://smith.langchain.com/datasets\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2717 Error creating dataset: {e}\")\n",
    "else:\n",
    "    print(\"No trace dataset to save. Skipping dataset creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Filter Examples\n",
    "\n",
    "Here are some useful filter patterns for selecting traces:\n",
    "\n",
    "```python\n",
    "# Filter by tags (AND condition)\n",
    "filter='has(tags, \"production\") and has(tags, \"v2\")'\n",
    "\n",
    "# Filter by feedback score\n",
    "filter='eq(feedback_score, 1)'  # Only positive feedback\n",
    "filter='gte(feedback_score, 0.8)'  # Score >= 0.8\n",
    "\n",
    "# Filter by metadata\n",
    "filter='eq(metadata.user_type, \"premium\")'\n",
    "\n",
    "# Filter by error status\n",
    "filter='eq(error, false)'  # Only successful runs\n",
    "\n",
    "# Combine multiple conditions\n",
    "filter='has(tags, \"production\") and eq(error, false) and gte(feedback_score, 0.8)'\n",
    "\n",
    "# Filter by date range\n",
    "start_time=datetime(2024, 1, 1)\n",
    "end_time=datetime(2024, 1, 31)\n",
    "```\n",
    "\n",
    "For more filter syntax, see: [LangSmith Filtering Documentation](https://docs.smith.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Use Manual Test Dataset or Trace Dataset\n",
    "\n",
    "Choose which dataset to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which dataset to use\n",
    "# If you have trace_dataset with data, use it; otherwise use manual test_data\n",
    "if trace_dataset and len(trace_dataset) > 0:\n",
    "    test_data = trace_dataset\n",
    "    print(f\"Using trace dataset with {len(test_data)} examples\")\n",
    "else:\n",
    "    # Manual test dataset (fallback)\n",
    "    test_data = [\n",
    "        {\n",
    "            \"input\": \"What is the capital of France?\",\n",
    "            \"expected_output\": \"Paris\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is 2 + 2?\",\n",
    "            \"expected_output\": \"4\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Who wrote Romeo and Juliet?\",\n",
    "            \"expected_output\": \"William Shakespeare\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is the largest planet in our solar system?\",\n",
    "            \"expected_output\": \"Jupiter\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is the chemical symbol for gold?\",\n",
    "            \"expected_output\": \"Au\"\n",
    "        }\n",
    "    ]\n",
    "    print(f\"Using manual test dataset with {len(test_data)} examples\")\n",
    "\n",
    "# Display first example\n",
    "print(\"\\nExample test case:\")\n",
    "print(f\"Input: {test_data[0]['input'][:100]}...\" if len(test_data[0]['input']) > 100 else f\"Input: {test_data[0]['input']}\")\n",
    "print(f\"Expected: {test_data[0]['expected_output'][:100]}...\" if len(test_data[0]['expected_output']) > 100 else f\"Expected: {test_data[0]['expected_output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation on Both Models\n",
    "\n",
    "We'll run each model on the test dataset and measure both correctness and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_evaluation(model, test_data, model_name):\n",
    "    \"\"\"\n",
    "    Run evaluation for a single model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_data):\n",
    "        print(f\"Running {model_name} - Test {i+1}/{len(test_data)}\", end=\"\\r\")\n",
    "        \n",
    "        # Measure latency\n",
    "        message = HumanMessage(content=test_case[\"input\"])\n",
    "        response, latency = measure_latency(model.invoke, [message])\n",
    "        \n",
    "        # Extract output\n",
    "        output = response.content\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluation = combined_evaluator(\n",
    "            output=output,\n",
    "            expected=test_case[\"expected_output\"],\n",
    "            latency=latency,\n",
    "            latency_threshold=5.0\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"input\": test_case[\"input\"],\n",
    "            \"expected\": test_case[\"expected_output\"],\n",
    "            \"output\": output,\n",
    "            \"correct\": evaluation[\"correct\"],\n",
    "            \"latency\": latency,\n",
    "            \"performance\": evaluation[\"performance\"],\n",
    "            \"overall_pass\": evaluation[\"overall_pass\"]\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n{model_name} evaluation complete!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations\n",
    "print(\"Starting Model 1 evaluation...\")\n",
    "results_model_1 = run_model_evaluation(model_1, test_data, \"GPT-3.5\")\n",
    "\n",
    "print(\"\\nStarting Model 2 evaluation...\")\n",
    "results_model_2 = run_model_evaluation(model_2, test_data, \"GPT-4\")\n",
    "\n",
    "# Combine results\n",
    "all_results = results_model_1 + results_model_2\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\u2713 All evaluations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name in [\"GPT-3.5\", \"GPT-4\"]:\n",
    "    model_data = df_results[df_results[\"model\"] == model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Correctness Rate: {model_data['correct'].mean():.1%}\")\n",
    "    print(f\"  Average Latency: {model_data['latency'].mean():.3f}s\")\n",
    "    print(f\"  Overall Pass Rate: {model_data['overall_pass'].mean():.1%}\")\n",
    "    print(f\"  Latency Range: {model_data['latency'].min():.3f}s - {model_data['latency'].max():.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results table\n",
    "print(\"\\nDetailed Results:\")\n",
    "display_columns = [\"model\", \"input\", \"correct\", \"latency\", \"performance\"]\n",
    "df_results[display_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Comparison: GPT-3.5 vs GPT-4', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Correctness comparison\n",
    "correctness_data = df_results.groupby('model')['correct'].mean()\n",
    "axes[0, 0].bar(correctness_data.index, correctness_data.values, color=['#3498db', '#e74c3c'])\n",
    "axes[0, 0].set_title('Correctness Rate', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Correctness Rate')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "for i, v in enumerate(correctness_data.values):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Average latency comparison\n",
    "latency_data = df_results.groupby('model')['latency'].mean()\n",
    "axes[0, 1].bar(latency_data.index, latency_data.values, color=['#3498db', '#e74c3c'])\n",
    "axes[0, 1].set_title('Average Latency', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Latency (seconds)')\n",
    "for i, v in enumerate(latency_data.values):\n",
    "    axes[0, 1].text(i, v + 0.05, f'{v:.3f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Latency distribution\n",
    "for model_name in ['GPT-3.5', 'GPT-4']:\n",
    "    model_latencies = df_results[df_results['model'] == model_name]['latency']\n",
    "    axes[1, 0].hist(model_latencies, alpha=0.6, label=model_name, bins=10)\n",
    "axes[1, 0].set_title('Latency Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Latency (seconds)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Overall pass rate\n",
    "pass_rate_data = df_results.groupby('model')['overall_pass'].mean()\n",
    "axes[1, 1].bar(pass_rate_data.index, pass_rate_data.values, color=['#3498db', '#e74c3c'])\n",
    "axes[1, 1].set_title('Overall Pass Rate (Correct + Fast)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Pass Rate')\n",
    "axes[1, 1].set_ylim([0, 1.1])\n",
    "for i, v in enumerate(pass_rate_data.values):\n",
    "    axes[1, 1].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Save the evaluation results to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = \"../data/evaluation_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"\u2713 Results exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Setting up model evaluation with LangSmith\n",
    "- Comparing two models on correctness and latency\n",
    "- Visualizing comparison results\n",
    "- Exporting results for further analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Customize the evaluation**: Modify the `combined_evaluator` function to match your specific needs\n",
    "2. **Load real datasets**: Connect to your LangSmith datasets\n",
    "3. **Add more models**: Compare additional models or providers\n",
    "4. **Advanced metrics**: Implement custom evaluation metrics\n",
    "5. **Automated reporting**: Set up scheduled evaluations and reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}