{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Version Evaluation with LangSmith\n",
    "\n",
    "This notebook demonstrates how to evaluate and compare two AI models/providers in terms of correctness and latency using LangSmith.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Set up your virtual environment (see README.md)\n",
    "2. Copy `.env.example` to `.env` and fill in your API keys\n",
    "3. Install dependencies: `pip install -r requirements.txt`\n",
    "\n",
    "## Overview\n",
    "\n",
    "This evaluation will:\n",
    "- Load a dataset from LangSmith\n",
    "- Run two different models on the same inputs\n",
    "- Measure correctness and latency\n",
    "- Compare results and generate visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path for imports\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('')), 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import custom utilities\n",
    "from src.config import validate_config\n",
    "from src.utils import measure_latency, calculate_metrics\n",
    "from src.evaluators import combined_evaluator\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    validate_config()\n",
    "    print(\"✓ Configuration validated successfully\")\n",
    "except ValueError as e:\n",
    "    print(f\"✗ Configuration error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain and LangSmith\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "print(\"✓ LangSmith client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Models to Compare\n",
    "\n",
    "We'll compare two different OpenAI models, but you can easily adapt this to compare different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure two models to compare\n",
    "model_1 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    tags=[\"model-1\", \"gpt-3.5\"]\n",
    ")\n",
    "\n",
    "model_2 = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0.7,\n",
    "    tags=[\"model-2\", \"gpt-4\"]\n",
    ")\n",
    "\n",
    "print(\"Model 1: GPT-3.5 Turbo\")\n",
    "print(\"Model 2: GPT-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create or Load Test Dataset\n",
    "\n",
    "For this example, we'll create a simple test dataset. In practice, you would load this from LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test dataset\n",
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"expected_output\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is 2 + 2?\",\n",
    "        \"expected_output\": \"4\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"expected_output\": \"William Shakespeare\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the largest planet in our solar system?\",\n",
    "        \"expected_output\": \"Jupiter\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the chemical symbol for gold?\",\n",
    "        \"expected_output\": \"Au\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Test dataset loaded: {len(test_data)} examples\")\n",
    "\n",
    "# Display first example\n",
    "print(\"\\nExample test case:\")\n",
    "print(f\"Input: {test_data[0]['input']}\")\n",
    "print(f\"Expected: {test_data[0]['expected_output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation on Both Models\n",
    "\n",
    "We'll run each model on the test dataset and measure both correctness and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_evaluation(model, test_data, model_name):\n",
    "    \"\"\"\n",
    "    Run evaluation for a single model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_data):\n",
    "        print(f\"Running {model_name} - Test {i+1}/{len(test_data)}\", end=\"\\r\")\n",
    "        \n",
    "        # Measure latency\n",
    "        message = HumanMessage(content=test_case[\"input\"])\n",
    "        response, latency = measure_latency(model.invoke, [message])\n",
    "        \n",
    "        # Extract output\n",
    "        output = response.content\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluation = combined_evaluator(\n",
    "            output=output,\n",
    "            expected=test_case[\"expected_output\"],\n",
    "            latency=latency,\n",
    "            latency_threshold=5.0\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"input\": test_case[\"input\"],\n",
    "            \"expected\": test_case[\"expected_output\"],\n",
    "            \"output\": output,\n",
    "            \"correct\": evaluation[\"correct\"],\n",
    "            \"latency\": latency,\n",
    "            \"performance\": evaluation[\"performance\"],\n",
    "            \"overall_pass\": evaluation[\"overall_pass\"]\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n{model_name} evaluation complete!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations\n",
    "print(\"Starting Model 1 evaluation...\")\n",
    "results_model_1 = run_model_evaluation(model_1, test_data, \"GPT-3.5\")\n",
    "\n",
    "print(\"\\nStarting Model 2 evaluation...\")\n",
    "results_model_2 = run_model_evaluation(model_2, test_data, \"GPT-4\")\n",
    "\n",
    "# Combine results\n",
    "all_results = results_model_1 + results_model_2\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n✓ All evaluations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name in [\"GPT-3.5\", \"GPT-4\"]:\n",
    "    model_data = df_results[df_results[\"model\"] == model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Correctness Rate: {model_data['correct'].mean():.1%}\")\n",
    "    print(f\"  Average Latency: {model_data['latency'].mean():.3f}s\")\n",
    "    print(f\"  Overall Pass Rate: {model_data['overall_pass'].mean():.1%}\")\n",
    "    print(f\"  Latency Range: {model_data['latency'].min():.3f}s - {model_data['latency'].max():.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results table\n",
    "print(\"\\nDetailed Results:\")\n",
    "display_columns = [\"model\", \"input\", \"correct\", \"latency\", \"performance\"]\n",
    "df_results[display_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Comparison: GPT-3.5 vs GPT-4', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Correctness comparison\n",
    "correctness_data = df_results.groupby('model')['correct'].mean()\n",
    "axes[0, 0].bar(correctness_data.index, correctness_data.values, color=['#3498db', '#e74c3c'])\n",
    "axes[0, 0].set_title('Correctness Rate', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Correctness Rate')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "for i, v in enumerate(correctness_data.values):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Average latency comparison\n",
    "latency_data = df_results.groupby('model')['latency'].mean()\n",
    "axes[0, 1].bar(latency_data.index, latency_data.values, color=['#3498db', '#e74c3c'])\n",
    "axes[0, 1].set_title('Average Latency', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Latency (seconds)')\n",
    "for i, v in enumerate(latency_data.values):\n",
    "    axes[0, 1].text(i, v + 0.05, f'{v:.3f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Latency distribution\n",
    "for model_name in ['GPT-3.5', 'GPT-4']:\n",
    "    model_latencies = df_results[df_results['model'] == model_name]['latency']\n",
    "    axes[1, 0].hist(model_latencies, alpha=0.6, label=model_name, bins=10)\n",
    "axes[1, 0].set_title('Latency Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Latency (seconds)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Overall pass rate\n",
    "pass_rate_data = df_results.groupby('model')['overall_pass'].mean()\n",
    "axes[1, 1].bar(pass_rate_data.index, pass_rate_data.values, color=['#3498db', '#e74c3c'])\n",
    "axes[1, 1].set_title('Overall Pass Rate (Correct + Fast)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Pass Rate')\n",
    "axes[1, 1].set_ylim([0, 1.1])\n",
    "for i, v in enumerate(pass_rate_data.values):\n",
    "    axes[1, 1].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Save the evaluation results to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = \"../data/evaluation_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"✓ Results exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Setting up model evaluation with LangSmith\n",
    "- Comparing two models on correctness and latency\n",
    "- Visualizing comparison results\n",
    "- Exporting results for further analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Customize the evaluation**: Modify the `combined_evaluator` function to match your specific needs\n",
    "2. **Load real datasets**: Connect to your LangSmith datasets\n",
    "3. **Add more models**: Compare additional models or providers\n",
    "4. **Advanced metrics**: Implement custom evaluation metrics\n",
    "5. **Automated reporting**: Set up scheduled evaluations and reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
